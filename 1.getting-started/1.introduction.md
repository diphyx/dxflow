---
title: Introduction
description: Learn what dxflow is and how it can help you manage and orchestrate your data & compute workflows across different computing environments
navigation:
    icon: i-hugeicons:bulb-charging
---

## What is dxflow?

`dxflow` is a comprehensive distributed computing engine that provides a unified interface for managing and orchestrating data & compute workflows across different computing environments.

::callout{type="info"}
**In short**: dxflow turns any machine you can access into a first-class citizen of your computational fleet, with one installer, a stable URL, and a unified interface for jobs, logs, and data.
::

### Key Features

::card-group
  ::card{title="Universal Deployment" icon="i-hugeicons:server"}
  Deploy on any infrastructure: cloud VMs, GPU nodes, on-premise clusters, or even your laptop
  ::

  ::card{title="Unified Interface" icon="i-hugeicons:interface"}
  Consistent CLI, REST API, and web UI across all environments
  ::

  ::card{title="Container Orchestration" icon="i-hugeicons:container"}
  Native Docker Compose integration with real-time monitoring
  ::

  ::card{title="Secure by Design" icon="i-hugeicons:security"}
  RSA key-pair authentication with fine-grained access control
  ::
::

## Architecture Components

dxflow consists of two main components:

### 1. Engine (Server)
The dxflow engine runs on your compute nodes and provides:
- **Compute Management**: Execute workflows on various platforms (Slurm, Kubernetes, Docker)
- **Resource Orchestration**: Manage CPU, GPU, and storage resources
- **Secure Access**: Authentication and authorization for remote access
- **Multi-Environment Support**: Works on closed systems, cloud platforms, and hybrid setups

### 2. Client Interfaces
Multiple ways to interact with your dxflow engines:
- **Command Line Interface (CLI)**: Full-featured terminal-based control
- **Web User Interface**: Visual dashboard for monitoring and management  
- **REST APIs**: Programmatic access for automation and integration

![Dxflow Layers](/assets/dxflow_layers.svg)

- **Infrastructure (Layer 1)** ‚Äì Raw compute: cloud VMs, GPU nodes, on‚Äëprem clusters, or even your laptop.

- **dxflow Engine (Layer 2)** ‚Äì A lightweight daemon you install on every compute‚Äëunit. It auto‚Äëregisters with the control plane, stages data, authenticates users, and translates high‚Äëlevel workflows into commands understood by the local scheduler/runtime.

- **Orchestration Runtimes (Layer 3)** ‚Äì The native workload managers already living on the node (Kubernetes, Slurm, Docker, Singularity, MPI, etc.). dxflow talks to these via pluggable adapters so your YAML never changes when you switch environments.

- **Application Layer (Layer 4)** ‚Äì The domain software scientists actually run: simulation codes, bioinformatics pipelines, ML training loops, post‚Äëprocessing scripts.

## Why Choose dxflow?

::card-group
  ::card{title="Unified Environment Management" icon="i-hugeicons:layers"}
  **Challenge**: Fragmented environments with different schedulers, security rules, and data paths  
  **Solution**: One runtime, one interface across all your infrastructure
  ::

  ::card{title="Streamlined Data Operations" icon="i-hugeicons:data-transfer"}
  **Challenge**: Manual data staging slows down research and development  
  **Solution**: Integrated file hub with resumable transfers and automatic cleanup
  ::

  ::card{title="Cost-Effective Scaling" icon="i-hugeicons:scaling"}
  **Challenge**: Static clusters are expensive for bursty workloads  
  **Solution**: Cloud-agnostic auto-scaling with spot instance support
  ::

  ::card{title="Enterprise Security" icon="i-hugeicons:shield"}
  **Challenge**: Complex security across VPNs and firewalls  
  **Solution**: Auto-provisioned TLS endpoints with RSA key-pair authentication
  ::
::

## Use Cases

dxflow is designed for anyone who needs to manage compute workflows across different environments:

::tabs
  ::tab-item{label="Data Science" icon="i-hugeicons:analytics"}
  - Large-scale data processing pipelines
  - ML model training across multiple GPUs
  - Jupyter notebook orchestration
  - Result visualization and sharing
  ::

  ::tab-item{label="Research Computing" icon="i-hugeicons:lab"}
  - Bioinformatics workflows
  - Computational biology simulations
  - High-performance computing jobs
  - Multi-site collaboration
  ::

  ::tab-item{label="DevOps & CI/CD" icon="i-hugeicons:workflow"}
  - Distributed testing environments  
  - Container orchestration
  - Deployment automation
  - Resource optimization
  ::

  ::tab-item{label="Edge Computing" icon="i-hugeicons:network"}
  - IoT data processing
  - Edge-to-cloud workflows
  - Distributed sensor networks
  - Real-time analytics
  ::
::

## Security & Privacy

dxflow implements enterprise-grade security to protect your computational resources and data:

### Authentication & Authorization

::alert{type="info"}
dxflow uses RSA public-key cryptography for secure authentication - a industry-standard approach for protecting distributed systems.
::

**Permission System:**

dxflow uses a composable permission model where users can have multiple permissions combined:

- üîç **READ_ONLY**: Restricts write operations (can be combined with other permissions)
- üñ•Ô∏è **SHELL**: Interactive terminal session management
- üìÅ **OBJECT**: File system and storage operations
- üîÑ **WORKFLOW**: Container and workflow orchestration  
- üåê **PROXY**: Network proxy service management
- üîó **BRIDGE**: Multi-engine connection management
- üñ•Ô∏è **PLATFORM**: Docker and platform operations
- üëë **MASTER**: Complete administrative control

Users can have combinations like `SHELL + READ_ONLY` (view shell sessions but cannot create/modify) or `WORKFLOW + OBJECT` (manage workflows and files).

### Network Security

::card-group
  ::card{title="TLS Encryption" icon="i-hugeicons:lock"}
  All communications secured with SSL/TLS certificates
  ::

  ::card{title="Zero-Trust Model" icon="i-hugeicons:verification"}
  Every connection authenticated and authorized
  ::

  ::card{title="Network Isolation" icon="i-hugeicons:firewall"}
  Unix sockets and secure HTTP endpoints
  ::
::

This security model allows you to safely manage workflows and files across different computing environments while maintaining strict access control.
